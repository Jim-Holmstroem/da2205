\documentclass{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{header}
\begin{document}

\generateheader{Final Assignment\\ \large ``Artificial Scientist and the tools
needed to get there''}


\section{Summary} %state what book and what part I focused on
    This essay will be focusing on the hardware limitations
    and the possibilities of machine learning.

% outline it in this order: (also check which statements/thoughts are mine vs.
% from the paper) remember to always have backup references.
%   
%   Hardware Limitations
%       -Introduction: Hardware isn't really catching up with what we want to
%       compute
%       -Stagnation in clock speed and memory bandwidth and such for one
%       processor unit
%       -Multicore CPU's
%       -Distributed data/compute systems, both colaborative clusters as well as
%       -supercomputers.(manycores that is computers running on 100core+).
%           Compute
%               Folding@home
%           data
%               ?
%       -(but the efficient can be as low as 1% <ref>)
%       -But this is still not optimal for machine learning.
%       -New non-von neumann<ref:?> architecture
%           -Look at GPGPU (general purpose computing on graphics processing unit)
%               -A "new" hardware which focuses more and data throughput<ref:>
%                and can gain a speedup of 10-100x times compare to running on a
%                CPU.(just check the examples out, don't link this: http://www.accelereyes.com/examples/case_studies)
%               -How many top500 computers are running with GPU's?
%           -Non-perfect calculations <ref:?>
%           -Neural processors <ref:Synaptic> would be optimal at least for the
%           algorithms simulating neural networks (examples of algorithms)
%      
%       -New hardware => new algorithms
%           -Clusters
%           -For Map-reduce<ref>
%           -Hadoop with mahong (machine learning algorithms implemented (and
%           redesigned) to run effectively on clusters)
%
%   Artificial Scientist
%       -Autonomous experimentation
%   
%       -Feedback to artificial intelligence itself (singularity)
%           -Genetic programming with a domain specific language like (example)
%       -Also improve the hardware it runs on
%       -I really hope we will be careful with the embodiment of these
%       artificial intelligence's so that we don't end up with Skynet<ref:
%       terminator> or a smart computer virus with a mind of their own<ref:?>.
%
%
%   Thoughts:
%
%

\section{The dawn of new hardware}
    Super scalability
    * Hardware isn't really catching up with the datasets, clock speed and
    memory bandwidth has
    stagnated and the only way to increase the computing power is to increase
    the number of cores or change the underlying architecture (as in GPU) but 
    this introduces a lot of new problems one has to re-implement all
    algorithms for the new setup efficiently which is non trivial in many
    cases. Mention cache-locks and

    * Hadoop (with Mahong ofc) is the most widely used framework for
    distributed processing on immense datasets across a cluster. 

    * Non-perfect hardware: many calculations need not to be perfect to be able
    to operate and some research into this has been done: //REF

\section{A new breed of scientists}
    Artificial Scientist: 
    * Use algorithms with active learning on real life experiments for example the
    microfluid one.
    * Use an artificial scientist to improve the artificial scientist, OMFG

    * Hardware is a bit  behind, need neurla processors or processors which do
    errors but the algorithms are errorcorrecting initself, which is closer to
    neuralprocessing in the neuronnets.
    * also need other ways of thinking dealing with massive-parallel, refer to
    bigdata-eventguy which had had the new parallel-databse idea. Most
    machinelearning algorithms is non-sequencial(is it called this?) but separating
    the problem in a map-reduce fasion isn't always trivial.
    * An important piece of the puzzle is to get machinelearning to be scaleable in
    deeplearning(is this correct usage of deeplearning?) so that we don't have to
    generate features byhand but instead have raw input with minimal preprocessing
    (perhaps only log-scaling some raw input you know have a exponential behavour.

    * Or redesigning the hardware it is running on to be optimal to .. (recursive
    call)

\end{document}
