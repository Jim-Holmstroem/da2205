\documentclass{article}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{hyperref}
\usepackage{header}
\usepackage{natbib}
\usepackage[utf8]{inputenc}
\begin{document}

\generateheader{Final Assignment\\ \large ``Artificial Scientists and the
needed paradigm shift to get there''}

\section{Summary}
    This essay will be focusing on the current software and hardware limitations
    towards the development of new machine learning techniques a specially the
    artificial scientist. Much of the claims and discussions will be based on
    the article ``Towards 2020 Science'' from Microsoft Research.\cite{ms2020}

    It seems that we are currently standing even closer to the brink of a new 
    computer revolution. New machine learning techniques are starting to be 
    developed and set off into production. Cognitive computing is taking form.
    At least two big companies I have 
    been working with, Ericsson and Oracle, are starting to consider using 
    machine learning for tasks like for example abnormality detection for 
    inhumanly complex systems with multiple soft
    states, classify the different states or different automatic utilities for
    testing software. This among other things results in
    the need of getting massive amounts of computing power.
    This hinder is nowadays much more focused on how to 
    distribute the computations to more computation units or sometimes use
    graphic cards but this is not a long time solution. We basically need a new
    computation paradigm for machine learning to get them as good as human
    performance in problems like object recognition or planing.

%    SUMMARY ON ARTIFICIAL SCIENTIST 


\section{The dawn of new hardware}
    The current processing capabilities for computers has lately degraded to a
    sub-exponential growth which is starting to have consequences such as 
    restricting the use of some algorithms as well as setting limits 
    to the processable size of datasets.
    This sub-exponential growth in computing has been a historical indicator
    that a new paradigm is on it's way which should be the case this time as
    well or at least show that there is a need for one.
    %<ref the_five_paradigm_changes_in_computing>
    
    The issue with most current hardware systems is their heavy and 
    na\"{i}ve use of the von Neumann architecture. Basically the throughput 
    for this types of architecture has for physical reasons started to reach 
    the end of the line performance wise.

    The duck-tape solution to this has been keeping the von Neumann
    architecture, with it's limiting bandwidth, and just throw in more cores 
    and/or more machines in parallel. This might work well in some big data 
    applications where the problems is easily map-reducible but perform really 
    badly in many other problems.\cite{mapreduce} Also this type of scaling 
    becomes very space and power exhaustive since you have the need for
    cooling of all the cores. 

    The only other solution is to just drop the overused von Neumann 
    architecture and go for something which is much more throughput oriented. 
    In the 2020 paper\cite{ms2020} foresighted \footnote{1 and 2 years before 
    the official announcement of CUDA resp. openCL.} the use of graphical 
    processing units (GPU) for general computing (GPGPU). General problems in 
    this case is basically problems with non graphical nature like
    solving differential equations, FFT or matrix operations. 
    High end GPU's nowadays have around 1500 stream processors in parallel 
    which can perform one type of operation (kernel) on a 1500-element-chunk of 
    the data in parallel at a time. This together with the heavy use 
    of a read-only memory which removes some of the problems with memory locks
    or write-after-write and that all the buses are separated for
    maximum throughput, they can in fact for many suitable cases not only scale
    almost linearly\footnote{Up to the GPU reaches the memory or instruction
    throughput max for the device which is much higher then for a CPU in the
    same price class.} on the number of cores but also have a much more 
    effective throughput on the data crunching compared to ordinary CPU's 
    which almost never manages.

    Both the scaling up with ordinary CPU's into multi- or manycore as well as
    the GPGPU parallelization requires new libraries utilizing new programming 
    paradigms.

    For the paradigm shift for software part,
    one possible solution instead of the old classical object oriented
    programming mostly used today would be to use a much more higher level and
    pure functional programming language such as
    for example Haskell. Some data driven companies has
    already conformed to this and are using it as their main language.
    \footnote{
        \href{http://www.janestreet.com}{Jane Street},
        \href{http://www.ericsson.com}{Ericsson},
        \href{http://www.campanja.com}{Campanja},
    }
    but in my opinion more companies needs to follow the functional train and
    instead focus to have libraries and the language itself run much faster.
    The inherited properties that makes functional programming languages 
    preferable is for example the lack of state 
    which makes it lack execution order which in turn makes it
    trivial to convert it to run it in parallel in a multi-/manycore cluster
    or on a GPU, since it already run in parallel on a semantical level, called
    implicit parallelization\cite{something}.
    \footnote{The statelessness in functional programming also makes it easier 
    to debug as well as much harder to introduce bugs, but that is another 
    story.} 
    Both also the extremely high level of the language with
    compositions and higher order functions built-in.\cite{haskell}

    But I still think that an entirely new architecture needs to be developed 
    even if the GPU solves many of the throughput bottlenecks.
    
    The probably the easiest way in trying to imitate neural behavior, 
    which in many of the hardest cases outperforms computers, is to actually
    imitate neural processing. But a question still remains how closely should
    we imitate neural processing, do we need spiking networks with simulated
    chemical behavior or could one just stay with special purpose circuits 
    for convolution networks.

    vissa bra saker som bara egenskaper spiking har, saker den l√∂ser, ref nips2012

    A new paradigm that recently has emerged is neuromorphic
    computing which basically tries to mimic neural functions with electronics.
    
    The freshest and most promising according to me is the 
    neural processing unit (NPU) which both IBM\cite{synapse} and
    Intel\cite{intelneuro} are starting to develop. Both the approaches avoids 
    being just a monstrous special purpose VHDL-logic chip, which we already 
    have seen a few of, and can thus be thought of as a new paradigm in itself 
    going one step further then simple neuromorphic computing but still being 
    one.

    http://www.research.ibm.com/new-era-of-computing.shtml 

    Just as for GPGPU's the usage of a NPU's requires a new programming 
    paradigm to operate, not much information on the NPU's nor their 
    development tools have been released at the moment, but I can speculate 
    that it would require some variant of logical- or/and functional 
    programming. Also the algorithms that fits into the NPU's needs to be 
    greatly rewritten.
    
    Another rather new and interesting neumorphic device is the
    dynamic vision sensor (DVS) which basically has asynchronous event based
    readout by spiking on color changes.\cite{dvs} This in contrast to ordinary
    cameras which only readout entire image for each timestep. 
    The DVS works much more like the human retina and
    these properties makes it easier to do 3D reconstruction as well as easily
    being really fast without using large bandwidth nor much power. A results
    of the extreme sparsity of data one can get more then 50000fps
    \footnote{In normal conditions, it cannot of course reach this extreme fps
    if the neurons spikes constantly.}
    over such a comparably low bandwidth interface as USB.

    In addition to all this some more radical improvements in learning
    algorithms has to be made as well. A very recent shift towards deep
    learning algorithms in the machine learning community that probably can be
    described as a paradigm shift. One cannot say for sure until after the
    breakthrough and this issue is much discussed in the machine learning
    community. For example it was foresighted that kernel-methods was going to
    solve everything and before that neural networks.
   
    Much of the recent success in deep learning is due to the new found 
    computing power from GPU's
    \footnote{And Google's 10k computer strong cluster of course.} 
    as I mentioned before as well as a new look on deep learning. A great 
    example on the later is the new usage of the dropout algorithm on deep 
    networks, more specifically a convolution network, which not only 
    outperforms the rest on the podium by xx\% compared to yy\% on the ImageNet 
    dataset.\cite{imagenet}
    But does it without using any type of ad hoc methods except for log scaling 
    some of the features compare to the competing algorithms that had a long 
    list of state of the art ad hoc's.\cite{dropout} This shows some real 
    promise for deep learning networks.

\section{A new breed of scientists}
    The need of new hardware becomes really apperant both in
    theory and practice when trying to create a general intelligence.
    A few really recent and in many ways successful tries has been made
    like for example: Google's unsupervised catdetector %REF catdetector
    %IBM simulation Synapse (distinguish this with the NPU Synapse)
    These tries has been ran on large clusters but would gain a
    significant efficiency boost by being ran on a more natural habitat namely 
    the NPU. 


    \bibliographystyle{plain}
    \bibliography{refs}

\end{document}
